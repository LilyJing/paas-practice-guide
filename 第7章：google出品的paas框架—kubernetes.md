# **第7章：Google出品的PaaS框架—Kubernetes**

## Kubernetes概览

根据Kubernetes官网的介绍，Kubernetes是一个开源的容器化应用的自动化部署、扩容和管理系统。同时Kubernetes是构建在Google近10年通过容器进行各种生产活动经验的基础之上的。下面这张图描述了Kubernetes的架构和工作流程。可以看出Kubernetes就是为了在一个物理机或者虚拟机的集群中调度和运行容器应用而设计的，通过Kubernetes系统我们可以帮助开发者摆脱对主机的依赖，从一个面向主机的基础设施转向一个面向容器的基础设施进而最大化容器带来的好处。同时Kubernetes系统引入了很多新的概念，例如Pod、Service、RC等，我们将在后面的内容中结合Kubernetes的使用场景展开这些概念，总之他们可以很好的适应当前的技术栈，通过对这些能力的组合使用我们可以实现很多常见的应用生产要求，比如：

- 应用健康性检查,
- 应用副本控制,
- 应用水平扩展
- 服务发现
- 负载均衡,
- 滚动升级、蓝绿发布
- …………

## Kubernetes提供的运行时环境

Kubernetes系统本身是一套容器的调度和编排系统，所以Kubernetes理论上来说是不限制运行时的，Kubernetes系统引入了Pod的概念，Pod是Kubernetes资源管理和调度的最小单元，由一组容器组成，在Kubernetes中每个Pod有自己的IP、CPU、内存、端口等资源，Pod中的容器则共享这部分资源。

## Kubernetes中的后端服务

目前Kubernetes可以以两种方式提供后端服务，一种是按照Open Service Broker API规范（<https://github.com/openservicebrokerapi/servicebroker>，CloudFoundry的ServiceBroker也是按照该规范来实现）实现的`service-catalog`，service-catalog在Kubernetes中增加了`Broker`、`ServiceClass`、`Instance`、`Binding`四种资源来实现对后端服务的管理，并使开发者可以订购后端服务并在自己的应用中使用这些后端服务，这是与而另一种是基于在ApiServer中注册第三方API方式实现的`Operator`。Operater本质上是一种Controller，在Kubernetes 1.2中提供了注册第三方资源的功能，我们可以很方便的在API Server中增加新的资源，而以往只能通过修改Kubernetes的一系列代码才能增加新的资源，当我们在Kubernetes中注册了新的资源之后，我们就可以自己实现一个Controller来跟踪第三方资源的变化，进而完成相应的操作。Operator就是按照这个流程来实现后端服务的管理，比如CoreOS实现的ETCD Operater（<https://github.com/coreos/etcd-operator>），通过这个Operater我们可以在Kubernetes集群中创建相应第三方资源，之后我们就可以通过Operater来创建、调整ETCD集群，实现对ETCD集群的数据备份和恢复以及进行升级等操作。

## 在Kubernetes中完成服务发现

在介绍Kubernetes的服务发现机制之前，我们先来介绍Kubernetes的一个重要概念—**标签（Label）**。标签是由一组k-v定义，标签在Kubernetes内部运行和运营维护过程中都起到至关重要的作用。与标签最直接相关的功能是**查询器（Selector）**，查询器支持通过标签来查询满足给定条件的对象。例如查询有`env=prd`标签的Pod。

为了在Kubernetes中完成服务发现，Kubernetes引入了一个Service的概念，是以IP或者域名方式为一组Pod提供负载均衡，Service有自己的端口，Service通过标签来确定自己要代理的Pod，当Service被访问时，Service就会把流量转发到相应的Pod。目前Service有userspace和iptables两种工作模式，userspace是模式是通过进程来实现Service到Pod的流量转发，Iptables模式是通过iptables的规则来实现流量转发，目前官方推荐和默认使用的iptables模式，可以在安装时或者安装后通过修改Kubernetes节点的配置来启用userspace模式。两种方式各有优劣，基于Iptables规则的流量转发是一次性的，这就需要所有的Pod都能随时接收请求，如果有一个Pod因为某些原因无法正常工作，而此时恰巧Service又轮询到这个Pod，那这次请求就失败了，而userspace模式则可以重试请求直到找一个可用的Pod为止。基于userspace的流量转发在效率上来讲要比Iptables模式慢。总的来说要让Service能够很好的进行流量转发就要很好的利用Kubernetes提供的可用性管理工具，这些可用性管理工具可以帮助维护Service代理的Pod列表或者重启Pod。

与服务发现相关的还有Kubernetes的网络设计，Kubernetes的网络设计主要是从三个方面来考虑，第一是所有的容器都可以不通过NAT方式来进行通信，第二所有的节点也可以不通过NAT方式来和所有的容器进行通信，第三是容器内外的IP一致。之所以这样做，一是能够减轻系统复杂性，另外可以让应用更容易的迁移到Kubernetes平台上来，而这正是其他一些平台所不具备的，例如CloudFoundry系统就会统一分配引用的端口，应用需要在启动时获取CF分配的端口，然后使用这个端口来启动服务，否则CF会认为应用没有正常启动而终止应用启动过程。同时由于端口是在应用启动时分配的，那么服务和服务之间的相互访问就会成问题。但是网络本身并不包含在Kubernetes系统中，而是通过一系列的网络插件进行容器网络管理，例如Openvswitch、flannel、calico等。这些网络插件都可以很好的与Kubernetes系统配合，具体使用那个网络插件工具可以根据具体情况来选择使用。

## Kubernetes中的可用性管理

在Kubernetes系统中可用性的管理可以分为两大部分，第一部分是**副本控制器**（RC，replication controller），第二部分是可用性探测（healthcheck）。其中副本控制器是Kubernetes**控制管理器（Controller manager）**的一部分，而控制管理器本身是一个无限循环的进程，Controllermanager是一组用来提升Kubernetes系统运行自动化和鲁棒性的组件集合，他们共同的特性就是维护系统对某一对象的当前状态与期望状态的一致，副本控制器就会维护特定应用在系统中启动的Pod实例数量来达到提升应用可用性的目的。比如我们需要RC来帮我们维持web前端应用有3个Pod实例，当由于运行着1个Pod的物理机或者虚拟机节点关机后，当前Kubernetes系统中就只剩2个Pod在运行，RC就会在一个轮询周期内发现少了1个Pod，RC就会通知Api Server重新启动1个Pod来满足我们对web前端的Pod实例数要求。类似的还有endpoints controller, namespace controller, and serviceaccounts controller，他们都大大增加了Kubernetes系统的自动化能力，而减轻了开发人员的工作量。

Kubernetes副本控制器让一个应用有足够的Pod实例数，来分散系统风险，提升应用的可用性，可用性探测则是跟踪每个Pod容器运行是否正常的手段，可用性探测可以通过HTTP请求、执行指定命名、探测Socket端口三种来判断容器是否正常运行，也就是如果HTTP请求的返回HTTP代码为200，执行命令的返回值为0或者可以连接到指定的Socket端口，Kubernetes就判断容器执行正常。

## 在Kubernetes中进行编排

在上面几部分介绍的和应用部署相关的概念，比如Pod、Service、RC等，都可以被定为在Kubernetes模板（template）里，通过template我们可以预先定义好一个应用的若干微服务的启动容器、副本数量、可用性检查条件、服务发现等要素，同时模板还支持参数，比如副本数量、应用名称等，这样就可以让模板更具通用性。

通过模板我们就可以很快把应用部署到其他Kubernetes集群或者命名空间下，基于Kubernetes的包管理工具Helm可以让我们像使用yum、apt-get等包管理工具一样快速在Kubernetes集群中安装应用和服务。在Helm中安装的包被称为`chart`，chart中包含了所有在Kubernetes中部署应用和服务的所有要素，同时Helm也提供了仓库（repository，与YUM repo的作用一样）来集中管理chart的配置，我们可以通过Helm在仓库中检索、查看包信息，选择合适的包安装到Kubernetes集群中。

如果我们已经使用了docker compose这样的编排工具，我们可以通过一个叫Kompose的工具把compose的yaml文件转换成Kubernetes`deployment`、`service`对象，这样就可以把通过compose启动的应用编排迁移到Kubernetes中来。

## **IaaS** **资源管理和调度**

在Kubernetes中对IaaS资源的调配和调度是由Kubernetes Master中的**调度器（Scheduler）**来完成的，调取器主要是来负责为需要启动的Pod在集群范围内指定启动节点。调度器并不会直接修改Pod的定义，而是通过创建Pod和Node的绑定关系来指定Pod的启动节点。Kubernetes系统默认的调度器主要是通过3步操作来完成节点的选择：

1. 过滤节点，首先从集群当前节点中过滤那些不符合Pod定义的节点，例如端口、文件系统、CPU、内存、节点标签等。
2. 对过滤后的节点进行打分，根据调度规则，例如从节点当前资源利用率角度打分，当前节点资源利用率越低的节点打分越高；从均衡节点间负载角度打分；从Pod在节点间的分布情况来打分，当前节点上归属于同一Service的Pod数量越少打分越高。除此以外还可以基于节点的标签进行亲和请和反亲和性的设置。例如出于更高的应用可用性角度出发，我们要把应用的Pod不但要调度到不同的节点，还要让这些节点归属于不同的机架，这样就可以事先给每个节点增加归属机架标签，然后通过对节点上的机架标签设置反亲和性调度规则来尽可能的把Pod分散到不同的机架上。
3. 确定最合适的节点。

上面我们介绍Kubernetes的调度器是如何工作的。下面我们来介绍一下Kubernetes如何管理每个节点上的可分配内存、CPU资源以及能够启动的Pod的数量的。

如果在没有特殊设置的情况下每个节点的可分配资源与当前节点的配置是相同的。但是这样极可能因为主机资源耗尽而影响kubelet和操作系统本身的工作，所以Kubernetes在kubelet的启动参数中增加了`kube-reserved`和`system-reserved`两个参宿来为kubelet和操作系统运行预留一定的资源。这样就把节点的容量和可调度资源区分开了，调度器会使用节点的可调度资源信息来完成调度相关的逻辑。

我们再来看看每个节点可调度的Pod数量的设置，当前版本的Kubernetes在默认情况下可以在每个节点上调度110个Pod。但是如果节点性能不好或者整个容器网络并没有规划足够的容器Ip地址，那么就可以对默认参数进行修改`max-pods`参数来减少每个节点可以调度的Pod数。

在Kubernetes节点运行一段时间以后就产生很多待清理的垃圾，包括容器镜像、容器产生的日志、Pod使用的临时持久化卷，都会占用大量的Kubernetes节点的存储资源，所以必须要定期进行清理。Kubernetes本身可以通过Kubelet的`image-gc-high-threshold`和`image-gc-low-threshold`两个参数来设置清理容器镜像的存储使用率阈值。对于容器运行的日志，特别是Docker可以设置容器运行日志为循环日志，这样就可以让容器日志只占用固定的大小，而新的日志会覆盖旧的日志，对于重要应用的日志则可以通过fluentd、logstash、heka等工具采集并汇总、分析。对于Pod挂载的临时持久化卷（emptyDir模式）则可以给节点上挂载一个块独立的文件系统进行存放，这样就算文件系统满了也不会影响操作系统的运行。文件系统的mount点可以在kubelet的启动参数`volumeDirectory`中查到。

## Kubernetes的容器数据持久化设计

就目前容器技术的特点来说，容器只能提供临时性的存储，这与主机所能提供的持久、耐用的存储有很大的不同，也就是因为这以往PAAS平台都提倡设计无状态的应用和服务，不过随着容器技术的不断推广和普及，在我们为容器技术带来的诸多好处而欲罢不能的时候，如何让更好的通过容器技术来运行有状态和持久化服务已经被提到了日程上来，Docker卷和卷插件的出现就是将有状态应用作为容器一等公民的开始。

Kubernetes对存储卷也有自己的考虑，它将存储系统抽象为持久化卷（PV，persistent volumes）、持久化声明（PVC，persistent volume claims）以及物理卷（volumes），之所以进行这样的抽象是因为在实际生产过程中，除了开发者会使用Kubernetes进行应用的开发、部署以外，还有系统管理员和系统运维人员来进行进行系统运营、运维操作。而Kubernetes对存储系统进行的抽象正好对应着这些角色的日常工作，开发者通过Yaml、Json通过PVC向Kubernetes系统提出用持久化的需求，而系统运维人员则维护着物理存储系统，例如Glusterfs、Ceph、NFS等，他们在存储系统中划分出一定容量的存储卷（volumes），同时Kubernetes系统管理员使用PV把存储卷编目到Kubernetes系统中，当Kubernetes收到开发者的持久化卷需求后就会在已经编目的存储卷中选择一个合适卷，并通过相应的存储插件讲存储卷挂载到容器中。

与Kubernetes底层网络系统类似，Kubernetes通过插件方式接入底层存储系统，目前Kubernetes可以根据IAAS所能提供的存储能力使用包括Amazon EBS、Ceph、Glusterfs、NFS、Flocker以及vSphere volumes在内的很多种存储方案，在这里尽管Kubernetes通过对存储卷的抽象和存储卷插件的使用在一定程度上标准化了容器存储的使用，但是不同的存储方案任然会对容器的使用，特别是对容器的调度，带来一定的影响。最突出的是在那种类似AmazonEBS的块存储或者主机文件系统上，由于AmazonEBS只能被一台Amazon虚机使用，当我们把一个AmazonEBS绑定到一个应用时，我们通过RC来增加应用Pod的副本数时，新增的Pod实际上都（因为Pod要使用amazonEBS,而一个AmazonEBS又只能被一个Amazon虚机使用的缘故）被调度到一个节点之上，这实际上并没有达到负载均衡或者高可用目的。当然要解决这个问题就需要通过Kubernetes的StatfulSet、DeamonSet之类的新功能来解决，我们将在后面进行介绍。下面我们介绍Kubernetes的PV和PVC。

就像我们之前介绍的PV和PVC将划分了Kubernetes系统管理员和开发者两类角色的工作界面，系统管理员需要负责提前为开发者在系统中准备持久化卷形成资源池，而开发者只需在系统中声明对存储的需求，实际上PV和PVC的关系与Node和Pod的关系是一样的，只不过PV、PVC解决的是开发者对数据持久化的需求，而Node、Pod解决的是开发者对计算、网络资源的需求。

![kubernetes_pvc](http://thenewstack.io/wp-content/uploads/2016/09/Kubernetes_PVC.png)

Source: Steve Watt, Red Hat

通过PV、PVC对持久化卷的抽象，我们可以在Kubernetes系统把数据持久化卷的生命周期划分为5个阶段，分别是：

1. 供给，管理员从指定的存储系统中创建PV，供给可以是手工的也可以是自动，手工方式就是管理员提前准备好各种规格的存储并编目到Kubernetes系统中，这种方式实际上对开发者或者系统管理员都不是特别友好，比如开发者有10Gi的存储需求而系统管理员只准备了5Gi的卷，显然系统无法将两者匹配起来，而反过来系统管理员准备了10Gi的卷而开发者只有5Gi的需求，或者管理准备了10块持久化卷而开发者只有2个持久化需求，这实际上都对系统资源带来了极大的浪费，因为Kubernetes系统在存储插件的配合和支持下提供了自动的供给手段，例如Kubernetes的存储插件可以支持在AmazonEBS、OpenStack Cinder、GCE PD、GlusterFS、Ceph RBD等文件系统上自动的创建物理盘。这样就极大的减轻了管理员的工作量，并且能更好的满足开发者多样化的需求。不过随着自动化供给的引入，社区对Kubernetes存储子系统进行了一定程度的重构，特别为了支持各存储插件在各后端存储上进行自动化的供给而引入了StorageClass概念，同时PVC可以根据标签来选择PV。
2. 绑定，开发者向Kubernetes提出存储需求（PVC），而系统选择一个可以满足要求的持久化卷并和需求进行绑定；
3. 使用，开发者在Pod中使用持久化卷，将PVC映射到容器的指定目录上；
4. 释放，当开发者不再有存储需求时，可以通过删除PVC来释放存储；
5. 回收，当与PV绑定的PVC被删除后，系统会根据管理员在创建PV时选定回收策略来对PV进行相应的操作。


## Kubernetes租户管理

在了解了Kubernetes的一些基本概念以后，我们来看看Kubernetes是如何管理集群里的资源的，相比与Mesos的资源供给模式，Kubernetes的资源管理接近于资源申请模式，Kubernetes的资源申请是定义在Pod中的，目前Kubernetes支持两种参数来设置Pod的资源用量，一个是Limits，一个是Requests，Limits代表了Pod可以使用的最大资源，Requests代表了Pod可以使用的最小资源，可以分别对CPU和内存设置Limits和Requests。

Limits和Requests两个参数的使用受到当前命名空间的配额（Quota）和LimitsRaange参数的影响，Quota决定了当前命名空间中所有Pod的总资源用量，LimitsRange则决定了单个Pod的Limits和Requests参数的设置范围，比如如果不想让单个Pod占用了当前命名空间的所有资源就可以通过LimitRange来设置单个Pod可以使用的资源上限。同时LimitRange还提供了Pod的Limits和Requests参数的默认值，因为一旦为当前命名空间设置了Quota参数，Limits和Requests参数就变成了必填参数，我们可以通过设置默认值来让系统自动填写这些参数，而不用根据环境的不同（例如生产环境和测试环境）设置不同的参数。这样就可以让一套编排文件能适应更多的Kubernetes系统。
